selector_to_html = {"a[href=\"#creating-datasets\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Creating datasets<a class=\"headerlink\" href=\"#creating-datasets\" title=\"Link to this heading\">#</a></h2><h3>GLAM Workbench notebooks<a class=\"headerlink\" href=\"#glam-workbench-notebooks\" title=\"Link to this heading\">#</a></h3>", "a[href=\"#documentation\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Documentation<a class=\"headerlink\" href=\"#documentation\" title=\"Link to this heading\">#</a></h2><p>These sections of the Trove Data Guide explain how to access text from different parts of Trove:</p>", "a[href=\"#software-packages\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">Software packages<a class=\"headerlink\" href=\"#software-packages\" title=\"Link to this heading\">#</a></h3>", "a[href=\"#data-sources\"]": "<h1 class=\"tippy-header\" style=\"margin-top: 0;\"><span class=\"section-number\">26.1. </span>Data sources<a class=\"headerlink\" href=\"#data-sources\" title=\"Link to this heading\">#</a></h1><h2>Documentation<a class=\"headerlink\" href=\"#documentation\" title=\"Link to this heading\">#</a></h2><p>These sections of the Trove Data Guide explain how to access text from different parts of Trove:</p>", "a[href=\"../../other-digitised-resources/oral-histories/accessing-data.html#oral-histories-transcripts\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Transcripts and summaries<a class=\"headerlink\" href=\"#transcripts-and-summaries\" title=\"Link to this heading\">#</a></h2><p>Each oral history record has a single text file combining summaries and transcripts for every session of the interview. The urls used to download this file have the pattern:</p><p><code class=\"docutils literal notranslate\"><span class=\"pre\">https://nla.gov.au/tarkine/listen/download/transcript/[NLA.OBJ</span> <span class=\"pre\">ID]</span></code></p>", "a[href=\"../../other-digitised-resources/periodicals/accessing-data.html#digitised-periodicals-data-text\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Text<a class=\"headerlink\" href=\"#text\" title=\"Link to this heading\">#</a></h2><p>There are three ways of getting OCRd text from periodicals:</p>", "a[href=\"#pre-harvested-datasets\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Pre-harvested datasets<a class=\"headerlink\" href=\"#pre-harvested-datasets\" title=\"Link to this heading\">#</a></h2>", "a[href=\"../../newspapers-and-gazettes/data/titles.html#newspapers-data-titles-text\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><a class=\"toc-backref\" href=\"#id11\" role=\"doc-backlink\">Title text</a><a class=\"headerlink\" href=\"#title-text\" title=\"Link to this heading\">#</a></h2><p>With the exception of some Government Gazettes which are available as <a class=\"reference internal\" href=\"../../accessing-data/downloadable-datasets.html\"><span class=\"doc std std-doc\">bulk downloads</span></a>, there\u2019s no direct way of accessing all the text of a title. You\u2019d need to use the <code class=\"docutils literal notranslate\"><span class=\"pre\">/result</span></code> endpoint to assemble a collection of articles and then aggregate the OCRd text from the individual articles. This could be done <a class=\"reference external\" href=\"https://glam-workbench.net/trove-harvester/\">issue by issue</a>, or by setting the <code class=\"docutils literal notranslate\"><span class=\"pre\">l-title</span></code> facet without a search query, and then <a class=\"reference internal\" href=\"../../accessing-data/how-to/harvest-complete-results.html\"><span class=\"doc std std-doc\">harvesting the complete result set</span></a>.</p><p>Depending on the title, this could take a significant amount of time and generate a large amount of data. You might want to use the <a class=\"reference external\" href=\"https://wragge.github.io/trove-newspaper-harvester/\">Trove Newspaper &amp; Gazette Harvester</a> for a job like this.</p>", "a[href=\"#glam-workbench-notebooks\"]": "<h3 class=\"tippy-header\" style=\"margin-top: 0;\">GLAM Workbench notebooks<a class=\"headerlink\" href=\"#glam-workbench-notebooks\" title=\"Link to this heading\">#</a></h3>", "a[href=\"../../newspapers-and-gazettes/data/issues.html#issue-text\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><a class=\"toc-backref\" href=\"#id10\" role=\"doc-backlink\">Issue text</a><a class=\"headerlink\" href=\"#issue-text\" title=\"Link to this heading\">#</a></h2><p>To get the full text of an issue you can <a class=\"reference internal\" href=\"#articles-in-an-issue\"><span class=\"std std-ref\">search for articles</span></a> within that issue and aggregate the text of all the individual articles. You\u2019ll need to set the <code class=\"docutils literal notranslate\"><span class=\"pre\">include</span></code> parameter to <code class=\"docutils literal notranslate\"><span class=\"pre\">articleText</span></code> to add the OCRd text to the results. Here\u2019s an example that collects the article texts from an issue and uses them to create a wordcloud using the <a class=\"reference external\" href=\"https://github.com/amueller/word_cloud\">WordCloud package</a>.</p>", "a[href=\"../../newspapers-and-gazettes/data/articles.html#newspapers-data-articles-text\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\">Article text<a class=\"headerlink\" href=\"#article-text\" title=\"Link to this heading\">#</a></h2><p>Trove provides the full text of articles for download. This makes it possible to use natural language processing and other computational methods to analyse the contents of newspapers.</p><p>There are actually three sources of newspaper article text:</p>", "a[href=\"../../newspapers-and-gazettes/data/pages.html#newspapers-data-pages-text\"]": "<h2 class=\"tippy-header\" style=\"margin-top: 0;\"><a class=\"toc-backref\" href=\"#id14\" role=\"doc-backlink\">Page text</a><a class=\"headerlink\" href=\"#page-text\" title=\"Link to this heading\">#</a></h2><p>To get the full text content of a page, you need to aggregate the content of all the articles published on that page. The method is much the same as that <a class=\"reference internal\" href=\"#get-page-identifier-from-search\"><span class=\"std std-ref\">described above</span></a> \u2013 you search for articles on a specific page by specifying the newspaper, date and page. You also need to set <code class=\"docutils literal notranslate\"><span class=\"pre\">include</span></code> to <code class=\"docutils literal notranslate\"><span class=\"pre\">articleText</span></code> to add the full text to the article records. You can then collect the text content of each article. Here\u2019s an example that collects the article texts from a page and uses them to create a wordcloud using the <a class=\"reference external\" href=\"https://github.com/amueller/word_cloud\">WordCloud package</a>.</p>"}
skip_classes = ["headerlink", "sd-stretched-link", "sd-rounded-pill"]

window.onload = function () {
    for (const [select, tip_html] of Object.entries(selector_to_html)) {
        const links = document.querySelectorAll(`article.bd-article ${select}`);
        for (const link of links) {
            if (skip_classes.some(c => link.classList.contains(c))) {
                continue;
            }

            tippy(link, {
                content: tip_html,
                allowHTML: true,
                arrow: true,
                placement: 'auto-start', maxWidth: 500, interactive: false,

            });
        };
    };
    console.log("tippy tips loaded!");
};
